{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26302276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting\n",
    "import plotly.express as px\n",
    "\n",
    "# data handeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# pre process\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import catboost\n",
    "\n",
    "# cross validation \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Set maximum display columns to 100\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# reproducabillitu\n",
    "import random\n",
    "random.seed(1140)\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() # allow progress apply with groupby\n",
    "\n",
    "# hyper-parmameter tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d8dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "test = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the data - very important for the catboost model with use_time option\n",
    "train = train.sort_values(by = [\"eventTimestamp\", \"deviceId\"])\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478d92b",
   "metadata": {},
   "source": [
    "### What do I see?\n",
    "- Time stamp is important for correct cross validation\n",
    "- Most features are categorical\n",
    "- c1 & c3 are categorical, c2 & c4 might be numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what size's are we dealing with?\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7a43a",
   "metadata": {},
   "source": [
    "### Lets start by looking at eventTimestamp, time is important (and if not considered .. dangerous) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978caaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train max time: {train.eventTimestamp.max()}\")\n",
    "print(f\"test min time: {test.eventTimestamp.min()}\")\n",
    "\n",
    "if train.eventTimestamp.max() > test.eventTimestamp.min():\n",
    "    print(\"not a time series! train time points are ahead..\")\n",
    "\n",
    "else:\n",
    "    print(\"time series! test time points are ahead..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db9936",
   "metadata": {},
   "source": [
    "### It looks like the test data and train data are not splitted by time... but..\n",
    "### Mabey each device is it's own time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_device = \"74f9b473fad\"\n",
    "\n",
    "device_df_train = train.query(\"deviceId == @example_device\")\n",
    "device_df_test = test.query(\"deviceId == @example_device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af28aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train max time: {device_df_train.eventTimestamp.max()}\")\n",
    "print(f\"test min time: {device_df_test.eventTimestamp.min()}\")\n",
    "\n",
    "if device_df_train.eventTimestamp.max() > device_df_test.eventTimestamp.min():\n",
    "    print(\"not a time series! train time points are ahead..\")\n",
    "\n",
    "else:\n",
    "    print(\"time series! test time points are ahead..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfa03b",
   "metadata": {},
   "source": [
    "##  A ha! got you. this device's data in the train set is priot to it's data in the test set\n",
    "## Let's see if that is the case for every deviceId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max train time stamp and min test time stemp per device \n",
    "max_time_train = train.groupby(\"deviceId\")[\"eventTimestamp\"].max().to_frame()\n",
    "min_time_test = test.groupby(\"deviceId\")[\"eventTimestamp\"].min().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all test data points are ahead of train data points\n",
    "device_max_train_min_test_df = max_time_train.merge(min_time_test, on=\"deviceId\", suffixes=(\"_train\", \"_test\"))\n",
    "device_max_train_min_test_df[\"is_time_series\"] = np.where(\n",
    "                    device_max_train_min_test_df.eventTimestamp_test - device_max_train_min_test_df.eventTimestamp_train >0,\n",
    "                    True,\n",
    "                    False)\n",
    "device_max_train_min_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384391c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"do all devices behave as time series: {device_max_train_min_test_df.is_time_series.all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9d10e",
   "metadata": {},
   "source": [
    "### Do we have unseen devices in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_devices_train = train[\"deviceId\"].nunique()\n",
    "total_devices_test = test[\"deviceId\"].nunique()\n",
    "\n",
    "new_devices = len(set(test[\"deviceId\"].unique()) - set(train[\"deviceId\"].unique()))\n",
    "\n",
    "print(f\"train device count: {total_devices_train}\")\n",
    "print(f\"test device count: {total_devices_test}\")\n",
    "print(f\"new device count in test: {new_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407082b5",
   "metadata": {},
   "source": [
    "### This is a time series problem with recpect to deviceId\n",
    "\n",
    "- Decision making: should I use time series split (with recpect to deviceId) or should I use groupKfold?\n",
    "- for cross-validation (hyperparameter tuning, model selection, feature selection) I will use time series split \n",
    "- That is for use of time based features.\n",
    "\n",
    "* Note - some devices in the test set are new (never seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1c135",
   "metadata": {},
   "source": [
    "### Let's look at the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nans in target: {train.winBid.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.winBid.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cd4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.query(\"winBid < 5\").winBid.plot(kind=\"hist\", bins=20, title=\"histogram: winBid < 5\", figsize = (20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3a1df",
   "metadata": {},
   "source": [
    "### From this plot output we see:\n",
    "- Most of the winning bids are below 0.5 while the most extreme bid won with 3405\n",
    "- No nan values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639031ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"unitDisplayType\")[\"winBid\"].mean().plot(kind=\"bar\",  figsize = (10,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9c642",
   "metadata": {},
   "source": [
    "### From this plot output we see:\n",
    "- banner is the cheapest to win\n",
    "- rewarded is the most expencive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"size\")[\"winBid\"].mean().sort_values().plot(kind=\"bar\",  figsize = (10,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dc478",
   "metadata": {},
   "source": [
    "### From this plot output we see:\n",
    "- The Cheapest size is 320x50\n",
    "- the most expensive size is: 480x320"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85dd4af",
   "metadata": {},
   "source": [
    "### Nan values check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train nans count: {train.isna().sum().sum()}\")\n",
    "print(f\"test nans count: {test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_isna = train.isna().sum()\n",
    "train_isna[train_isna > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e97189",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_isna = test.isna().sum()\n",
    "test_isna[test_isna > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a8f1d",
   "metadata": {},
   "source": [
    "### I will use mode imputation for countryCode, connectionType nan values.\n",
    "- Note: For this test I will only handle these nans, in a real project I will implement fillna for every column in case such nans will appear in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be59b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic (baseline) pre_processing\n",
    "def default_pre_process(df):\n",
    "    to_drop = [\"eventTimestamp\", \"deviceId\"]\n",
    "    to_label_encode = [c for c in df if df[c].dtype == \"object\" and c not in to_drop]\n",
    "    le = LabelEncoder()\n",
    "    le_dict = {}\n",
    "    for col in to_label_encode:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        le_dict[col] = le\n",
    "    \n",
    "    for fillna_col in [ \"countryCode\", \"connectionType\"]:\n",
    "        most_common = df[fillna_col].value_counts().idxmax()\n",
    "        df[fillna_col] =  df[fillna_col].fillna(most_common)\n",
    "    return df.drop(to_drop, axis = 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_fit_predict_score(X_train, y_train, X_val, y_val,parms=None):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # squared=False to make mse -> rmse\n",
    "    score = mean_squared_error(y_val, y_pred, squared=False) \n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c219a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main cross validation function\n",
    "df - dataframe to train & evaluate on\n",
    "save_train_indexs/ save_test_indexs - used for reproducabillity\n",
    "fit_func/pre_process_func/ feature_enigneering_func - what functions are used in the pipeline \n",
    "to_drop - columns to drop before training a model\n",
    "parms - hyperparameters to add\n",
    "\n",
    "returns:\n",
    "score - RMSE\n",
    "save_train_indexs, save_test_indexs -  used for reproducabillity\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cross_validation(df, save_train_indexs = None, save_test_indexs=None,fit_func=default_fit_predict_score, \n",
    "                     pre_process_func = default_pre_process, feature_enigneering_func=None, to_drop=[],\n",
    "                    parms = None):\n",
    "    if \"has_won\" not in to_drop:\n",
    "        to_drop.append(\"has_won\")\n",
    "        \n",
    "    X = df.drop(\"winBid\", axis = 1)\n",
    "    y = df[\"winBid\"]\n",
    "\n",
    "    N_FOLDS = 5\n",
    "    tscv = TimeSeriesSplit(n_splits = N_FOLDS)\n",
    "\n",
    "    if save_train_indexs is None:\n",
    "        save_train_indexs = []\n",
    "        save_test_indexs = []\n",
    "        save_indexs = True\n",
    "    else:\n",
    "        save_indexs = False\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in tqdm(enumerate(tscv.split(X))):\n",
    "        if save_indexs:\n",
    "            save_train_indexs.append(train_index)\n",
    "            save_test_indexs.append(test_index)\n",
    "\n",
    "        X_train = X.loc[train_index].copy()\n",
    "        X_val =  X.loc[test_index].copy()\n",
    "\n",
    "        y_train =  y[train_index]\n",
    "        y_val = y[test_index]\n",
    "\n",
    "        X_train = pre_process_func(X_train)\n",
    "        X_val = pre_process_func(X_val)\n",
    "\n",
    "        if feature_enigneering_func is not None:\n",
    "            X_train = feature_enigneering_func(X_train)\n",
    "            X_val =  feature_enigneering_func(X_val, is_train=False, train_set = X_train )\n",
    "    \n",
    "        X_train = X_train.drop(to_drop, axis = 1)\n",
    "        X_val = X_val.drop(to_drop, axis = 1)\n",
    "\n",
    "        fold_score = fit_func(X_train, y_train, X_val, y_val, parms)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    score = sum(scores) / N_FOLDS\n",
    "    print(f\"RMSE score: {round(score,2)} [$]\")\n",
    "    return score, save_train_indexs, save_test_indexs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67aba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, save_train_indexs, save_test_indexs= cross_validation(train.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e588e5",
   "metadata": {},
   "source": [
    "## Let's try to improve the initial score\n",
    "### Let's look at our categorical featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b838269",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [c for c in train if train[c].dtype == \"object\"]\n",
    "print(\"number of unique values\\n\")\n",
    "\n",
    "for col in cat_features:\n",
    "    print(f\"{col}: {train[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [c for c in train if train[c].dtype != \"object\" and c not in [\"winBid\", \"has_won\"]]\n",
    "numerical_features\n",
    "for col in numerical_features:\n",
    "    print(f\"{col}: {train[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfc7ee",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "\n",
    "### Categorical features decisions\n",
    "- Do not use use deviceId \n",
    "- use catboost (see modeling decision) \n",
    "- convert \"size\" to width, length and total (width * length)\n",
    "\n",
    "### Numerical features decisions\n",
    "- try treating c2 and c4 as categorical features\n",
    "- create new feature: sentPrice - bidFloorPrice,  total bids made (per device)\n",
    "\n",
    "### Time based features\n",
    "- for those features test will recieve last value per device, if exist - and will be maen imputed otherwise\n",
    "- has_won expanding mean per device \n",
    "- \"bids_speed\" = number of bids per device / (max time stemp - min time stemp)\n",
    "- \"bids_spees\" * has_won_expanding_mean\n",
    "\n",
    "### Modeling decision \n",
    "- use Catboost Regressor as a model - over 50% categorical features (mabey even more..)\n",
    "- use RMSE as Catboost's loss function\n",
    "- use hastime = True // time series support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32106854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This functions creates the following features:\n",
    "\n",
    "width - derived from left side of \"size\"\n",
    "length - derived from right side of \"size\"\n",
    "total - total add area\n",
    "\n",
    "bid_diff - bid sent - bid floor price\n",
    "total_bids - count bids made untill now, per device\n",
    "has_won_expanding_average - winning rate per device\n",
    "time_alive - time that this device is in the data\n",
    "bids_speed - count bids / time alive\n",
    "wins_speed - count wins/ time alive\n",
    "\n",
    "for the following features, the last value in the train set is used for the test set:\n",
    "\"total_bids\", \"has_won_expanding_average\", \"bids_speed\", \"wins_speed\", \"time_alive\"\n",
    "\"\"\"\n",
    "def iteration_1_feature_engineering(df, is_train = True, train_set = None):        \n",
    "    df[\"width\"] = df[\"size\"].str.split(\"x\",expand=True)[0].astype(\"int\")\n",
    "    df[\"length\"] = df[\"size\"].str.split(\"x\",expand=True)[1].astype(\"int\")\n",
    "    df[\"total\"] = df[\"width\"]  * df[\"length\"]\n",
    "\n",
    "    df[\"bid_diff\"] = df.sentPrice - df.bidFloorPrice\n",
    "    \n",
    "    if is_train:\n",
    "        # for covinient\n",
    "        df_gb_deviceId = df.groupby(\"deviceId\")\n",
    "        \n",
    "        df[\"total_bids\"] = df_gb_deviceId['has_won'].agg(\"cumcount\")\n",
    "        \n",
    "        # avoid leakage - only use values from the past by substracting current value\n",
    "        has_won_cumsum = df_gb_deviceId['has_won'].agg(\"cumsum\") - df[\"has_won\"]\n",
    "        \n",
    "        # avoid devied by 0 by adding 1\n",
    "        has_won_cumcount = df_gb_deviceId['has_won'].agg(\"cumcount\") + 1 \n",
    "        df['has_won_expanding_average'] = has_won_cumsum / has_won_cumcount\n",
    "        \n",
    "#         \"bids_speed\" = number of bids per device / time alive\n",
    "        \n",
    "        device_start_time = df_gb_deviceId[\"eventTimestamp\"].min()\n",
    "        time_alive = df[\"eventTimestamp\"] - device_start_time\n",
    "        \n",
    "        # + 1 to avoid dev by 0\n",
    "        df[\"time_alive\"] = df[\"eventTimestamp\"] - df[\"deviceId\"].map(device_start_time) + 1 \n",
    "        \n",
    "        \n",
    "        df[\"bids_speed\"] = df[\"total_bids\"] /  df[\"time_alive\"]\n",
    "        \n",
    "        df[\"wins_speed\"] = df[\"bids_speed\"] * df['has_won_expanding_average']\n",
    "\n",
    "    \n",
    "    else:\n",
    "        for col in [\"total_bids\", \"has_won_expanding_average\", \"bids_speed\", \"wins_speed\", \"time_alive\"]:\n",
    "            last_train_values = train_set.groupby(\"deviceId\")[col].agg('last')\n",
    "            default_value = last_train_values.mean()\n",
    "            col_default_dict = defaultdict(lambda: default_value, last_train_values.to_dict())\n",
    "\n",
    "            df[col] = df[\"deviceId\"].map(col_default_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using label encoder - using catboost instead.\n",
    "def iteration_1_pre_process(df):\n",
    "    for fillna_col in [ \"countryCode\", \"connectionType\"]:\n",
    "        most_common = df[fillna_col].value_counts().idxmax()\n",
    "        df[fillna_col] =  df[fillna_col].fillna(most_common)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a688c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using catboost - because of high categorical features fraction\n",
    "def iteration_1_fit_predict_score(X_train, y_train, X_val, y_val, parameters = None):\n",
    "    cat_features =  [c for c in X_train if X_train[c].dtype == \"object\"]\n",
    "    \n",
    "    parms = {\"cat_features\" : cat_features, \n",
    "            \"loss_function\" : \"RMSE\",\n",
    "            \"has_time\": True,\n",
    "             'iterations': 100,\n",
    "             'verbose' : 0}\n",
    "    \n",
    "    if parameters is not None:\n",
    "        for new_parm in parameters:\n",
    "            parms[new_parm] = parameters[new_parm]\n",
    "            \n",
    "    model = catboost.CatBoostRegressor(**parms)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # squared=False to make mse -> rmse\n",
    "    score = mean_squared_error(y_val, y_pred, squared=False) \n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_1_fit_predict(X_train, y_train, X_test, parameters = None):\n",
    "    cat_features =  [c for c in X_train if X_train[c].dtype == \"object\"]\n",
    "    \n",
    "    parms = {\"cat_features\" : cat_features, \n",
    "            \"loss_function\" : \"RMSE\",\n",
    "            \"has_time\": True,\n",
    "             'iterations': 100,\n",
    "             'verbose' : 0}\n",
    "    \n",
    "    if parameters is not None:\n",
    "        for new_parm in parameters:\n",
    "            parms[new_parm] = parameters[new_parm]\n",
    "            \n",
    "    model = catboost.CatBoostRegressor(**parms)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check iteration #1 score\n",
    "score, save_train_indexs, save_test_indexs= cross_validation(train.copy(), \n",
    "                                                             save_train_indexs, \n",
    "                                                             save_test_indexs,\n",
    "                                                             fit_func=iteration_1_fit_predict_score,\n",
    "                                                            pre_process_func=iteration_1_pre_process,\n",
    "                                                            feature_enigneering_func=iteration_1_feature_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacab71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# enter random column\n",
    "# later - any column less important than the random column will be removed\n",
    "\n",
    "train[\"random_column\"] =  np.random.rand(train.shape[0])\n",
    "test[\"random_column\"] =  np.random.rand(test.shape[0])\n",
    "\n",
    "y_train = train[\"winBid\"]\n",
    "\n",
    "X_train = iteration_1_pre_process(train)\n",
    "X_train = iteration_1_feature_engineering(X_train)\n",
    "\n",
    "X_test = iteration_1_pre_process(test)\n",
    "X_test = iteration_1_feature_engineering(X_test, is_train=False, train_set = X_train)\n",
    "\n",
    "\n",
    "\n",
    "_, final_model = iteration_1_fit_predict(X_train.drop([\"has_won\", \"winBid\"], axis = 1), y_train, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = final_model.get_feature_importance()\n",
    "feature_names = final_model.feature_names_\n",
    "\n",
    "\n",
    "# Create a dataframe for feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance using Plotly\n",
    "fig = px.bar(feature_importance_df, x='Feature', y='Importance', title='CatBoost Feature Importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns with less importance than random column\n",
    "random_col_importance = feature_importance_df.query(\"Feature == 'random_column'\")[\"Importance\"].values[0]\n",
    "to_drop = list(feature_importance_df.loc[feature_importance_df.Importance <= random_col_importance].Feature.unique())\n",
    "\n",
    "to_drop = [c for c in to_drop if c != \"deviceId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cv score without non important columns \n",
    "score, save_train_indexs, save_test_indexs= cross_validation(train.copy(), \n",
    "                                                             save_train_indexs, \n",
    "                                                             save_test_indexs,\n",
    "                                                             fit_func=iteration_1_fit_predict_score,\n",
    "                                                            pre_process_func=iteration_1_pre_process,\n",
    "                                                            feature_enigneering_func=iteration_1_feature_engineering,\n",
    "                                                            to_drop=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's down sample the data and use optuna for hyperparameter tuning\n",
    "\n",
    "train_optuna = train.sample(frac = 0.2)\n",
    "save_train_indexs = None\n",
    "save_test_indexs= None\n",
    "train_optuna = train_optuna.sort_values(by = [\"eventTimestamp\", \"deviceId\"])\n",
    "train_optuna = train_optuna.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed696257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    parms = {\n",
    "        'iterations': trial.suggest_int('iterations', 10, 400),\n",
    "        'depth': trial.suggest_int('depth', 2, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "    }\n",
    "\n",
    "    \n",
    "    score, save_train_indexs, save_test_indexs= cross_validation(train_optuna.copy(), \n",
    "                                 save_train_indexs=None, \n",
    "                                 save_test_indexs=None,\n",
    "                                 fit_func=iteration_1_fit_predict_score,\n",
    "                                pre_process_func=iteration_1_pre_process,\n",
    "                                feature_enigneering_func=iteration_1_feature_engineering,\n",
    "                                to_drop=to_drop,\n",
    "                                parms = parms)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, timeout= 60*60) # one hour \n",
    "\n",
    "# Print the best parameters and best score\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best score:', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # submission\n",
    "for col in [\"has_won\", \"winBid\"]:\n",
    "    if col not in to_drop:\n",
    "        to_drop.append(col)\n",
    "        \n",
    "to_drop_test = [c for c in to_drop if c not in [\"has_won\", \"winBid\"]]\n",
    "        \n",
    "y_train = train[\"winBid\"]\n",
    "\n",
    "X_train = iteration_1_pre_process(train)\n",
    "X_test = iteration_1_pre_process(test)\n",
    "\n",
    "X_train = iteration_1_feature_engineering(X_train)\n",
    "X_test =  iteration_1_feature_engineering(X_test, is_train=False, train_set = X_train )\n",
    "\n",
    "X_train = X_train.drop(to_drop, axis = 1)\n",
    "X_test = X_test.drop(to_drop_test, axis = 1)\n",
    "X_test[\"winBid\"], final_model = iteration_1_fit_predict(X_train, y_train, X_test,\n",
    "                                                       parameters = study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[[\"deviceId\", \"winBid\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[[\"deviceId\", \"winBid\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
